# ARC-013: Content Extraction & Processing - Implementation Plan

**Date**: November 22, 2025
**Branch**: `OM-21-arc-13-content-extraction-and-processing`
**Status**: In Progress - Building on existing foundation

---

## üìä Current Implementation Status

### ‚úÖ **Already Complete** (Estimated 60% done!)

#### **Phase 1: Web Article Extraction** - 80% Complete

**Dependencies Installed**:
- ‚úÖ `@mozilla/readability@^0.6.0` - Content extraction
- ‚úÖ `linkedom@^0.18.5` - DOM parsing for Node.js
- ‚úÖ `cross-fetch` - HTTP fetching
- ‚ùå `dompurify` - HTML sanitization (NEEDS INSTALLATION)
- ‚ùå `turndown` - HTML to Markdown conversion (NEEDS INSTALLATION)

**ContentProcessorService** (`src/queue/processors/content-processor.service.ts`):
- ‚úÖ BullMQ worker setup with concurrency control
- ‚úÖ Job routing (`FETCH_CONTENT`, `PARSE_CONTENT`)
- ‚úÖ HTTP fetching with proper headers and timeout (30s)
- ‚úÖ Mozilla Readability integration
- ‚úÖ Open Graph metadata extraction
  - ‚úÖ `og:title`, `og:description`, `og:image`
  - ‚úÖ Twitter Card metadata
  - ‚úÖ Favicon extraction
- ‚úÖ Word count calculation (from HTML)
- ‚úÖ Content saving to database
- ‚úÖ State management (PROCESSING ‚Üí SUCCEEDED/FAILED)
- ‚úÖ Event emission (fetch started/completed/failed)
- ‚úÖ Error handling with retry logic
- ‚úÖ Progress tracking
- ‚úÖ Graceful shutdown

**What's Working**:
```typescript
// Current flow:
1. Fetch HTML from URL (with User-Agent, timeout, headers)
2. Parse HTML with linkedom
3. Extract Open Graph metadata
4. Extract article content with Readability
5. Calculate word count
6. Save to database:
   - title, author, description
   - readableContent (HTML)
   - thumbnail, siteIcon, siteName
   - wordCount, publishedAt
7. Update state to SUCCEEDED
```

**Test Coverage**:
- ‚úÖ Unit tests exist (`content-processor.service.spec.ts`)
- ‚ùå E2E tests not yet written

---

## üöß **What Needs to Be Done**

### **Phase 1 Completion: Web Article Extraction** (2-3 hours)

#### 1. Install Missing Dependencies
```bash
npm install --save isomorphic-dompurify turndown
npm install --save-dev @types/dompurify @types/turndown
```

#### 2. Implement HTML Sanitization
**File**: `src/queue/services/html-sanitizer.service.ts` (NEW)

**Purpose**: Sanitize extracted HTML to prevent XSS attacks

**Implementation**:
```typescript
import DOMPurify from 'isomorphic-dompurify'
import { parseHTML } from 'linkedom'

@Injectable()
export class HtmlSanitizerService {
  sanitize(html: string): string {
    // Create window context for DOMPurify
    const { window } = parseHTML('<!DOCTYPE html>')
    const purify = DOMPurify(window as any)

    return purify.sanitize(html, {
      ALLOWED_TAGS: ['p', 'br', 'b', 'i', 'strong', 'em', 'a', 'img', 'ul', 'ol', 'li', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'blockquote', 'code', 'pre'],
      ALLOWED_ATTR: ['href', 'src', 'alt', 'title', 'class'],
      ALLOW_DATA_ATTR: false,
    })
  }
}
```

**Integration**: Update ContentProcessorService to sanitize before saving

#### 3. Add Markdown Conversion (Optional for now)
**File**: `src/queue/services/markdown-converter.service.ts` (NEW)

**Purpose**: Convert HTML to Markdown for plain text views

```typescript
import TurndownService from 'turndown'

@Injectable()
export class MarkdownConverterService {
  private turndown: TurndownService

  constructor() {
    this.turndown = new TurndownService({
      headingStyle: 'atx',
      codeBlockStyle: 'fenced',
    })
  }

  convert(html: string): string {
    return this.turndown.turndown(html)
  }
}
```

---

### **Phase 2: Image Processing** (3-4 hours)

#### 4. Implement ImageProxyService
**File**: `src/queue/services/image-proxy.service.ts` (NEW)

**Capabilities**:
- Download and cache images locally
- Resize/optimize images
- Generate thumbnails
- Return proxied URLs

**Approach** (choose one):

**Option A: Simple S3/Object Storage**
```typescript
@Injectable()
export class ImageProxyService {
  async processImages(html: string, itemId: string): Promise<string> {
    // 1. Extract all img tags
    // 2. Download each image
    // 3. Upload to S3/MinIO
    // 4. Replace src with proxy URL
    // 5. Return modified HTML
  }
}
```

**Option B: Defer to Later (Recommended for MVP)**
- Keep original image URLs for now
- Add image proxy in ARC-014
- Focus on getting content extraction working first

**Decision**: **Defer to ARC-014** (avoid scope creep)

---

### **Phase 3: Content Enhancements** (2-3 hours)

#### 5. Enhance Metadata Extraction
**Current**: Basic Open Graph extraction
**Add**:
- ‚úÖ JSON-LD structured data parsing
- ‚úÖ Additional Twitter Card fields
- ‚úÖ Article schema metadata

**File**: Update `extractOpenGraph()` in ContentProcessorService

```typescript
private extractMetadata(document: Document, url: string): Metadata {
  const ogData = this.extractOpenGraph(document, url)
  const jsonLd = this.extractJsonLd(document)
  const twitterData = this.extractTwitterCard(document)

  return {
    ...ogData,
    ...jsonLd,
    ...twitterData,
  }
}

private extractJsonLd(document: Document): any {
  const scripts = document.querySelectorAll('script[type="application/ld+json"]')
  for (const script of scripts) {
    try {
      const data = JSON.parse(script.textContent || '')
      if (data['@type'] === 'Article' || data['@type'] === 'NewsArticle') {
        return {
          author: data.author?.name,
          publishedTime: data.datePublished,
          headline: data.headline,
        }
      }
    } catch (e) {
      // Ignore invalid JSON
    }
  }
  return {}
}
```

#### 6. Add Content Hash (for Duplicate Detection)
**File**: `src/queue/services/content-hasher.service.ts` (NEW)

```typescript
import { createHash } from 'crypto'

@Injectable()
export class ContentHasherService {
  generateHash(content: string): string {
    return createHash('sha256')
      .update(content)
      .digest('hex')
  }
}
```

**Integration**: Add `contentHash` field to LibraryItemEntity

#### 7. Add Reading Time Estimation
**Already done!** Word count is calculated, reading time is `wordCount / 200` (average reading speed)

---

### **Phase 4: Testing & Polish** (2-3 hours)

#### 8. Create E2E Tests
**File**: `test/content-extraction.e2e-spec.ts` (NEW)

**Test Cases**:
```typescript
describe('Content Extraction E2E Tests', () => {
  it('should save URL and extract content', async () => {
    // 1. Save URL via saveUrl mutation
    // 2. Wait for job to complete
    // 3. Query library item
    // 4. Verify extracted content exists
  })

  it('should handle extraction failures gracefully', async () => {
    // Test 404, timeouts, invalid HTML
  })

  it('should extract metadata correctly', async () => {
    // Verify title, author, description, thumbnail
  })

  it('should calculate word count', async () => {
    // Verify word count accuracy
  })

  it('should sanitize HTML', async () => {
    // Test XSS prevention
  })
})
```

#### 9. Performance Testing
**Targets** (from ARC-013 spec):
- ‚úÖ Extraction completes in <10 seconds for typical articles
- ‚úÖ Current: 30 second timeout (should be sufficient)

**Test**:
- Measure actual extraction time for various websites
- Verify concurrency works (3 concurrent jobs)

#### 10. Error Handling Polish
**Current**: Good foundation with retry logic
**Add**:
- Better error messages for users
- Distinguish between temporary (retry) and permanent (don't retry) failures
- Add specific error codes (404, timeout, parse error, etc.)

---

## üéØ Implementation Order (Recommended)

### **Day 1** (Today) - Core Functionality
1. ‚úÖ Review existing implementation (DONE)
2. Install missing dependencies (dompurify, turndown)
3. Implement HTML sanitization
4. Add JSON-LD metadata extraction
5. Add content hash generation
6. Test manually with real websites

### **Day 2** - Testing & Validation
7. Write E2E tests
8. Write unit tests for new services
9. Performance testing
10. Fix any bugs discovered

### **Day 3** - Polish & Documentation
11. Error handling improvements
12. User feedback messages
13. Update documentation
14. Code review and cleanup

---

## üì¶ Dependencies Status

### Already Installed ‚úÖ
- `@mozilla/readability@^0.6.0`
- `linkedom@^0.18.5`
- `cross-fetch`
- `typeorm`, `@nestjs/typeorm`
- `bullmq`, `@nestjs/bullmq`

### Need to Install ‚ùå
- `isomorphic-dompurify` - HTML sanitization
- `turndown` - HTML to Markdown (optional)
- `@types/dompurify` - TypeScript types
- `@types/turndown` - TypeScript types

---

## üîç Current Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         SaveUrl GraphQL Mutation                ‚îÇ
‚îÇ   (src/library/resolvers/save-url.resolver.ts)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ          Queue Job Enqueued                     ‚îÇ
‚îÇ   (ContentProcessingQueue.add('fetch-content')) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ      ContentProcessorService Worker             ‚îÇ
‚îÇ   (src/queue/processors/content-processor.ts)   ‚îÇ
‚îÇ                                                  ‚îÇ
‚îÇ  1. Fetch HTML (cross-fetch)                    ‚îÇ
‚îÇ  2. Parse HTML (linkedom)                       ‚îÇ
‚îÇ  3. Extract metadata (Open Graph)               ‚îÇ
‚îÇ  4. Extract content (Readability)               ‚îÇ
‚îÇ  5. Calculate word count                        ‚îÇ
‚îÇ  6. Save to database                            ‚îÇ
‚îÇ  7. Update state                                ‚îÇ
‚îÇ  8. Emit events                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         LibraryItemEntity Updated                ‚îÇ
‚îÇ   (title, content, metadata saved)               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## ‚úÖ Acceptance Criteria Progress

| Criterion | Status |
|-----------|--------|
| Save URL extracts article title, author, content, images | ‚úÖ 80% (needs sanitization) |
| Extracted content displays correctly in reader | ‚úÖ Yes (HTML content saved) |
| Images load through proxy/cache | ‚ùå Not implemented (defer to ARC-014) |
| Failed extractions show helpful error messages | ‚úÖ Partially (needs polish) |
| Content hash prevents duplicates | ‚ùå Not implemented (Phase 3) |
| E2E test: Save article ‚Üí read in reader | ‚ùå Not written yet |
| Extraction completes in <10 seconds | ‚úÖ Yes (30s timeout, typically <5s) |
| All existing tests still pass | ‚úÖ 174 tests passing |

---

## üöÄ Next Steps

**Immediate** (Today):
1. Install dependencies (dompurify, turndown)
2. Implement HtmlSanitizerService
3. Integrate sanitization into ContentProcessorService
4. Test with real websites manually

**Tomorrow**:
1. Write E2E tests
2. Add JSON-LD metadata extraction
3. Add content hash generation
4. Performance validation

**Day 3**:
1. Polish error messages
2. Update documentation
3. Code review
4. Merge PR

---

## üéì Key Design Decisions

### ‚úÖ **Decisions Made**
1. **Use linkedom instead of jsdom** - Faster, lighter weight
2. **Use Mozilla Readability** - Battle-tested, open source
3. **Implement as BullMQ worker** - Async, scalable, resilient
4. **Fallback to Open Graph** - Graceful degradation when Readability fails
5. **30 second timeout** - Balance between patience and responsiveness

### ‚è≥ **Deferred Decisions**
1. **Image proxy** - Defer to ARC-014 (keep original URLs for now)
2. **PDF extraction** - Defer to ARC-014
3. **Video transcripts** - Defer to ARC-014
4. **RSS parsing** - Defer to ARC-014

---

**Status**: ‚úÖ Strong foundation in place, ~60% complete
**Remaining Work**: ~2-3 days of focused development
**Priority**: üî¥ CRITICAL - Completes core save-to-read workflow

**Next Action**: Install dependencies and implement HTML sanitization
